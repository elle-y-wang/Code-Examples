{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Fine-tuning on Risk-Factor Text (Melanoma / BCC / SCC / AnySC / KC)\n",
    "This notebook trains a binary classifier with HuggingFace Transformers on a TSV where each row is a participant:\n",
    "- `text` is the natural-language encoding of structured variables\n",
    "- labels: `label_melanoma`, `label_bcc`, `label_scc`, `label_any_skin_cancer`, `label_keratinocyte_cancer`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# 5-Fold (4 folds train + 1 fold validation) with Early Stopping\n",
    "# Model choices: BERT / BioBERT / ClinicalBERT (always fine-tune)\n",
    "# Safe loader for torch<2.6 (CVE) with safetensors-first & auto-fallback\n",
    "# ==========================================================\n",
    "# Optional installation:\n",
    "# !pip install torch transformers scikit-learn pandas numpy matplotlib safetensors\n",
    "\n",
    "import os, json, time, random, re, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, f1_score, accuracy_score\n",
    "\n",
    "from packaging.version import Version\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "try:\n",
    "    from transformers import get_linear_schedule_with_warmup\n",
    "except Exception:\n",
    "    get_linear_schedule_with_warmup = None  # Compatibility with very old versions\n",
    "\n",
    "# ------------------- Config -------------------\n",
    "TSV_PATH   = r\"D:\\All projects_250223\\Phototherapy_and_ AI\\Skin cancer dataset Kalia clinic\\bert_inputs.tsv\"   # Local path to the TSV input file\n",
    "LABEL_NAME = \"label_melanoma\"                 # Options: label_melanoma / label_bcc / label_scc / label_any_skin_cancer / label_keratinocyte_cancer\n",
    "\n",
    "# Choose a pretrained model (select one of the following):\n",
    "# \"bert\" -> bert-base-uncased   (has safetensors)\n",
    "# \"biobert\" -> dmis-lab/biobert-base-cased-v1.1       (no safetensors)\n",
    "# \"clinicalbert\" -> emilyalsentzer/Bio_ClinicalBERT   (no safetensors)\n",
    "MODEL_CHOICE = \"clinicalbert\"   # <<< Change model choice here: \"bert\" | \"biobert\" | \"clinicalbert\"\n",
    "\n",
    "MODEL_VARIANTS = {\n",
    "    \"bert\": \"bert-base-uncased\",\n",
    "    \"biobert\": \"dmis-lab/biobert-base-cased-v1.1\",\n",
    "    \"clinicalbert\": \"emilyalsentzer/Bio_ClinicalBERT\",\n",
    "}\n",
    "assert MODEL_CHOICE in MODEL_VARIANTS, f\"MODEL_CHOICE must be one of {list(MODEL_VARIANTS.keys())}\"\n",
    "MODEL_NAME = MODEL_VARIANTS[MODEL_CHOICE]\n",
    "\n",
    "# Whether to automatically fall back to bert-base-uncased when torch < 2.6\n",
    "# and the selected model does not provide safetensors weights\n",
    "\n",
    "AUTO_FALLBACK_TO_BERT = True\n",
    "\n",
    "MAX_LENGTH = 256\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS     = 200              # Set high; rely on early stopping to terminate training\n",
    "LR         = 2e-5\n",
    "WARMUP_RATIO = 0.06\n",
    "WEIGHT_DECAY = 0.01\n",
    "MAX_GRAD_NORM = 1.0\n",
    "\n",
    "SEED      = 42\n",
    "N_SPLITS  = 5\n",
    "PATIENCE  = 30                 # Stop training if validation metric does not improve for PATIENCE consecutive epochs\n",
    "OUTPUT_DIR_BASE = f\"./cv_models_foldval_{MODEL_CHOICE}\"\n",
    "os.makedirs(OUTPUT_DIR_BASE, exist_ok=True)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", DEVICE, \"| Requested model:\", MODEL_NAME, \"| torch:\", torch.__version__)\n",
    "\n",
    "# ------------------- Utils -------------------\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=256):\n",
    "        self.labels = labels.astype(int).tolist()\n",
    "        self.enc = tokenizer(\n",
    "            list(texts),\n",
    "            truncation=True,\n",
    "            padding=False,              # Do not pad here; padding is handled in collate_fn\n",
    "            max_length=max_length,\n",
    "            return_tensors=None\n",
    "        )\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self, idx):\n",
    "        item = {\n",
    "            'input_ids': torch.tensor(self.enc['input_ids'][idx], dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(self.enc['attention_mask'][idx], dtype=torch.long),\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "        if 'token_type_ids' in self.enc:\n",
    "            item['token_type_ids'] = torch.tensor(self.enc['token_type_ids'][idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Pad only variable-length sequence fields; labels are scalars and stacked directly.\n",
    "    \"\"\"\n",
    "    out = {'labels': torch.tensor([b['labels'] for b in batch], dtype=torch.long)}\n",
    "    for k in batch[0].keys():\n",
    "        if k == 'labels':\n",
    "            continue\n",
    "        seqs = [b[k] for b in batch]  # list[1D tensor]\n",
    "        out[k] = torch.nn.utils.rnn.pad_sequence(seqs, batch_first=True, padding_value=0)\n",
    "    return out\n",
    "\n",
    "def compute_metrics_from_probs(y_true, probs):\n",
    "    preds = (probs >= 0.5).astype(int)\n",
    "    out = {}\n",
    "    try: out[\"auroc\"] = float(roc_auc_score(y_true, probs))\n",
    "    except: out[\"auroc\"] = float(\"nan\")\n",
    "    try: out[\"auprc\"] = float(average_precision_score(y_true, probs))\n",
    "    except: out[\"auprc\"] = float(\"nan\")\n",
    "    out[\"f1\"] = float(f1_score(y_true, preds))\n",
    "    out[\"accuracy\"] = float(accuracy_score(y_true, preds))\n",
    "    return out\n",
    "\n",
    "def bar_plot_class_balance(df, label_name):\n",
    "    vc = df[label_name].value_counts(dropna=False).sort_index()\n",
    "    labels = [str(k) for k in vc.index]\n",
    "    vals   = [int(v) for v in vc.values]\n",
    "    plt.figure(figsize=(5,4))\n",
    "    plt.bar(labels, vals)\n",
    "    plt.title(f\"Class Balance: {label_name}\")\n",
    "    plt.xlabel(\"Class\"); plt.ylabel(\"Count\")\n",
    "    plt.show()\n",
    "    print(vc)\n",
    "\n",
    "# -------- Safe loader to handle torch<2.6 & CVE (prefer safetensors) --------\n",
    "def safe_load_tokenizer(model_id):\n",
    "    # Tokenizers are not affected by torch.load vulnerabilities\n",
    "    return AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "\n",
    "def _is_cve_torch_error(e: Exception) -> bool:\n",
    "    msg = str(e)\n",
    "    return \"vulnerability\" in msg.lower() and \"torch.load\" in msg.lower() or \"CVE-2025-32434\" in msg\n",
    "\n",
    "def safe_load_sequence_classifier(model_id, num_labels=2):\n",
    "    \"\"\"\n",
    "    1) Prefer loading safetensors weights\n",
    "    2) If loading fails due to torch<2.6 CVE issues and fallback is allowed, switch to bert-base-uncased (which provides safetensors)\n",
    "    3) Re-raise all other errors unchanged\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_id,\n",
    "            num_labels=num_labels,\n",
    "            use_safetensors=True,   # Important, looking for safetensors\n",
    "        )\n",
    "    except Exception as e:\n",
    "        if _is_cve_torch_error(e) and Version(torch.__version__) < Version(\"2.6\"):\n",
    "            # Indicates that the model lacks safetensors weights and local torch < 2.6\n",
    "            warn = (f\"[SAFELOAD] '{model_id}' may not provide safetensors weights, and your local torch={torch.__version__} < 2.6ï¼Œ\"\n",
    "                    f\"Due to CVE-2025-32434 loading bin weights cannot be performed safely.\")\n",
    "            if AUTO_FALLBACK_TO_BERT:\n",
    "                print(warn + \" utomatically falling back to 'bert-base-uncased'(which provides safetensors) to continue training\")\n",
    "                return AutoModelForSequenceClassification.from_pretrained(\n",
    "                    \"bert-base-uncased\", num_labels=num_labels, use_safetensors=True\n",
    "                )\n",
    "            else:\n",
    "                raise RuntimeError(warn + \" Please upgrade PyTorch to >= 2.6 or switch to a model that provides safetensors weights (e.g., bert-base-uncased).\") from e\n",
    "        else:\n",
    "            # Not triggered by the CVE; re-raise the original exception to aid debugging\n",
    "            raise\n",
    "\n",
    "# ------------------- Load data -------------------\n",
    "set_seed(SEED)\n",
    "df = pd.read_csv(TSV_PATH, sep=\"\\t\")\n",
    "assert \"text\" in df.columns, \"TSV must contain 'text'.\"\n",
    "assert LABEL_NAME in df.columns, f\"TSV must contain '{LABEL_NAME}'.\"\n",
    "df = df.dropna(subset=[LABEL_NAME]).copy().reset_index(drop=True)\n",
    "df[LABEL_NAME] = df[LABEL_NAME].astype(int)\n",
    "print(\"Columns:\", list(df.columns))\n",
    "print(\"Shape:\", df.shape)\n",
    "bar_plot_class_balance(df, LABEL_NAME)\n",
    "\n",
    "# ------------------- Train / Eval loops -------------------\n",
    "def train_one_epoch(model, loader, optimizer, scheduler, loss_fct, scaler=None):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch in loader:\n",
    "        for k in batch:\n",
    "            batch[k] = batch[k].to(DEVICE)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        if scaler is not None:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(**{k: batch[k] for k in batch if k != \"labels\"})\n",
    "                logits = outputs.logits  # [B,2]\n",
    "                loss = loss_fct(logits, batch[\"labels\"])\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)\n",
    "            scaler.step(optimizer)\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "            scaler.update()\n",
    "        else:\n",
    "            outputs = model(**{k: batch[k] for k in batch if k != \"labels\"})\n",
    "            logits = outputs.logits\n",
    "            loss = loss_fct(logits, batch[\"labels\"])\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)\n",
    "            optimizer.step()\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "\n",
    "        total_loss += loss.item() * batch[\"labels\"].size(0)\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_get_probs(model, loader):\n",
    "    model.eval()\n",
    "    probs = []\n",
    "    ys = []\n",
    "    for batch in loader:\n",
    "        for k in batch:\n",
    "            batch[k] = batch[k].to(DEVICE)\n",
    "        outputs = model(**{k: batch[k] for k in batch if k != \"labels\"})\n",
    "        logits = outputs.logits  # [B,2]\n",
    "        p = torch.softmax(logits, dim=1)[:,1].detach().cpu().numpy()\n",
    "        probs.append(p)\n",
    "        ys.append(batch[\"labels\"].detach().cpu().numpy())\n",
    "    probs = np.concatenate(probs, axis=0)\n",
    "    ys = np.concatenate(ys, axis=0)\n",
    "    return ys, probs\n",
    "\n",
    "def build_class_weight_loss(y_int, device):\n",
    "    # CrossEntropyLoss(weight=[w0,w1]) where weights are inverse frequency\n",
    "    pos = int(y_int.sum())\n",
    "    neg = int(len(y_int) - pos)\n",
    "    if pos == 0 or neg == 0:\n",
    "        return nn.CrossEntropyLoss().to(device), None\n",
    "    total = pos + neg\n",
    "    w0 = total / (2.0 * neg)\n",
    "    w1 = total / (2.0 * pos)\n",
    "    weights = torch.tensor([w0, w1], dtype=torch.float32, device=device)\n",
    "    return nn.CrossEntropyLoss(weight=weights).to(device), (w0, w1, pos, neg)\n",
    "\n",
    "# ------------------- Fold training with Early Stopping (validation = held-out fold) -------------------\n",
    "def train_fold(train_df, val_df, fold_id, tokenizer, model_name):\n",
    "    # Datasets and data loaders\n",
    "    ds_trn = TextDataset(train_df[\"text\"], train_df[LABEL_NAME], tokenizer, MAX_LENGTH)\n",
    "    ds_val = TextDataset(val_df[\"text\"],   val_df[LABEL_NAME],   tokenizer, MAX_LENGTH)\n",
    "\n",
    "    dl_trn = DataLoader(ds_trn, batch_size=BATCH_SIZE, shuffle=True,  collate_fn=collate_fn, num_workers=0)\n",
    "    dl_val = DataLoader(ds_val, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn, num_workers=0)\n",
    "\n",
    "    # Pretrained model with fine-tuning (always fine-tune; prefer safetensors)\n",
    "    model = safe_load_sequence_classifier(model_name, num_labels=2)\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    # scheduler\n",
    "    total_steps = EPOCHS * len(dl_trn)\n",
    "    if get_linear_schedule_with_warmup is not None:\n",
    "        warmup_steps = int(WARMUP_RATIO * total_steps)\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "    else:\n",
    "        scheduler = None\n",
    "\n",
    "    # Class-weighted loss (computed from training fold only)\n",
    "    loss_fct, cls_info = build_class_weight_loss(train_df[LABEL_NAME].values.astype(int), DEVICE)\n",
    "    if cls_info:\n",
    "        w0, w1, pos, neg = cls_info\n",
    "        print(f\"[Fold {fold_id}] Class weights -> w0={w0:.4f}, w1={w1:.4f} (neg={neg}, pos={pos})\")\n",
    "    else:\n",
    "        print(f\"[Fold {fold_id}] Class weights skipped (single-class in train).\")\n",
    "\n",
    "    # Automatic Mixed Precision (if available)\n",
    "    scaler = torch.cuda.amp.GradScaler() if torch.cuda.is_available() else None\n",
    "\n",
    "    # Early stopping based on validation AUPRC\n",
    "    best_metric = -np.inf\n",
    "    best_state = None\n",
    "    best_epoch = -1\n",
    "    bad_rounds = 0\n",
    "\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        t0 = time.time()\n",
    "        train_loss = train_one_epoch(model, dl_trn, optimizer, scheduler, loss_fct, scaler)\n",
    "        y_val, p_val = eval_get_probs(model, dl_val)\n",
    "        m_val = compute_metrics_from_probs(y_val, p_val)\n",
    "        elapsed = time.time() - t0\n",
    "        print(f\"[Fold {fold_id}] Epoch {epoch}/{EPOCHS} | train_loss={train_loss:.4f} | VAL AUPRC={m_val['auprc']:.4f} AUROC={m_val['auroc']:.4f} | {elapsed:.1f}s\")\n",
    "\n",
    "        # Use AUPRC for early stopping; fall back to AUROC if AUPRC is NaN\n",
    "        score = m_val[\"auprc\"]\n",
    "        if np.isnan(score):\n",
    "            score = m_val[\"auroc\"]\n",
    "\n",
    "        if score > best_metric:\n",
    "            best_metric = score\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "            best_epoch = epoch\n",
    "            bad_rounds = 0\n",
    "        else:\n",
    "            bad_rounds += 1\n",
    "\n",
    "        if bad_rounds >= PATIENCE:\n",
    "            print(f\"[Fold {fold_id}] Early stopping at epoch {epoch} (best epoch {best_epoch}, best metric {best_metric:.4f})\")\n",
    "            break\n",
    "\n",
    "    # Restore best-performing model parameters\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    # Evaluate best model on the validation fold\n",
    "    y_fold, p_fold = eval_get_probs(model, dl_val)\n",
    "    m_fold = compute_metrics_from_probs(y_fold, p_fold)\n",
    "    print(f\"[Fold {fold_id}] Best on VAL | AUPRC={m_fold['auprc']:.4f} AUROC={m_fold['auroc']:.4f} F1={m_fold['f1']:.4f} ACC={m_fold['accuracy']:.4f} | best_epoch={best_epoch}\")\n",
    "\n",
    "    return model, tokenizer, p_fold, m_fold\n",
    "\n",
    "# ------------------- Run 5-fold -------------------\n",
    "set_seed(SEED)\n",
    "y_all = df[LABEL_NAME].values\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "\n",
    "# Tokenizers are not affected by the torch.load CVE and can be loaded safely\n",
    "tokenizer = safe_load_tokenizer(MODEL_NAME)\n",
    "\n",
    "fold_metrics = []\n",
    "oof_probs = np.zeros(len(df), dtype=float)\n",
    "\n",
    "for fold_id, (trn_idx, val_idx) in enumerate(skf.split(np.zeros(len(y_all)), y_all), start=1):\n",
    "    print(f\"\\n======== Fold {fold_id}/{N_SPLITS} ========\")\n",
    "    # 4 folds for training, 1 fold for validation\n",
    "    df_train = df.iloc[trn_idx].reset_index(drop=True).copy()\n",
    "    df_valid = df.iloc[val_idx].reset_index(drop=True).copy()\n",
    "\n",
    "    model, tok, p_fold, m_fold = train_fold(df_train, df_valid, fold_id, tokenizer, MODEL_NAME)\n",
    "    fold_metrics.append(m_fold)\n",
    "\n",
    "    # Save the best model and tokenizer for this fold\n",
    "    out_dir = os.path.join(OUTPUT_DIR_BASE, f\"fold_{fold_id}\")\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    model.save_pretrained(out_dir)\n",
    "    tok.save_pretrained(out_dir)\n",
    "    with open(os.path.join(out_dir, \"fold_metrics.json\"), \"w\") as f:\n",
    "        json.dump(m_fold, f, indent=2)\n",
    "\n",
    "    # Store out-of-fold probabilities (val_idx corresponds to original df indices)\n",
    "    oof_probs[val_idx] = p_fold\n",
    "\n",
    "# Aggregate metrics across folds\n",
    "def agg(key):\n",
    "    vals = np.array([m[key] for m in fold_metrics], dtype=float)\n",
    "    return float(np.nanmean(vals)), float(np.nanstd(vals))\n",
    "\n",
    "print(\"\\n====== 5-fold Summary (VAL per fold) ======\")\n",
    "print(\"AUPRC : mean={:.4f}, std={:.4f}\".format(*agg(\"auprc\")))\n",
    "print(\"AUROC : mean={:.4f}, std={:.4f}\".format(*agg(\"auroc\")))\n",
    "print(\"F1    : mean={:.4f}, std={:.4f}\".format(*agg(\"f1\")))\n",
    "print(\"ACC   : mean={:.4f}, std={:.4f}\".format(*agg(\"accuracy\")))\n",
    "\n",
    "# OOF evaluation (predictions from each fold's validation set)\n",
    "y_true = df[LABEL_NAME].values.astype(int)\n",
    "oof_metrics = compute_metrics_from_probs(y_true, oof_probs)\n",
    "print(\"\\n====== OOF Metrics ======\")\n",
    "print(json.dumps(oof_metrics, indent=2))\n",
    "\n",
    "# Save OOF\n",
    "np.save(os.path.join(OUTPUT_DIR_BASE, f\"oof_probs_{LABEL_NAME}.npy\"), oof_probs)\n",
    "df.assign(oof_prob=oof_probs).to_csv(os.path.join(OUTPUT_DIR_BASE, f\"oof_{LABEL_NAME}.csv\"), index=False)\n",
    "print(f\"Saved models & OOF to: {OUTPUT_DIR_BASE}\")\n",
    "\n",
    "# ------------------- Ensemble Inference -------------------\n",
    "@torch.no_grad()\n",
    "def predict_proba_single_model(texts, model_path, max_length=MAX_LENGTH, device=DEVICE):\n",
    "    tok = AutoTokenizer.from_pretrained(model_path, use_fast=True)\n",
    "    mdl = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=2, use_safetensors=True).to(device).eval()\n",
    "    enc = tok(list(texts), truncation=True, padding=True, max_length=max_length, return_tensors=\"pt\")\n",
    "    for k in enc: enc[k] = enc[k].to(device)\n",
    "    logits = mdl(**enc).logits\n",
    "    probs = torch.softmax(logits, dim=1)[:,1].detach().cpu().numpy()\n",
    "    return probs\n",
    "\n",
    "def predict_proba_ensemble(texts, base_dir=OUTPUT_DIR_BASE, n_splits=N_SPLITS, max_length=MAX_LENGTH, device=DEVICE):\n",
    "    probs_list = []\n",
    "    for k in range(1, n_splits+1):\n",
    "        fold_dir = os.path.join(base_dir, f\"fold_{k}\")\n",
    "        if not os.path.isdir(fold_dir):\n",
    "            raise FileNotFoundError(f\"Missing model dir: {fold_dir}\")\n",
    "        probs_list.append(predict_proba_single_model(texts, fold_dir, max_length=max_length, device=device))\n",
    "    return np.vstack(probs_list).mean(axis=0)\n",
    "\n",
    "# #  Example usage:\n",
    "# sample_texts = [\"Age: 40s. Sex: male. ...\", \"Age: 60s. Sex: female. ...\"]\n",
    "# ens_probs = predict_proba_ensemble(sample_texts)\n",
    "# print(\"Ensemble probs:\", ens_probs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
